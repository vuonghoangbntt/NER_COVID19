{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/'\n",
    "DATA_TYPE = 'word'\n",
    "PADDING_TOKEN = 0\n",
    "UNKNOWN_TOKEN = 1\n",
    "START_SENTENCE = 2\n",
    "END_SENTENCE = 3\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "DROPOUT = 0.1\n",
    "EMBEDDING_SIZE = 200\n",
    "HIDDEN_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    sentences = []\n",
    "    entities = []\n",
    "    with open(data_path,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        sent = []\n",
    "        tok = []\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens)==0:\n",
    "                sentences.append(sent)\n",
    "                entities.append(tok)\n",
    "                sent=[]\n",
    "                tok=[]\n",
    "                continue\n",
    "            if len(tokens)!=2:\n",
    "                print(tokens)\n",
    "                continue\n",
    "            sent.append(tokens[0])\n",
    "            tok.append(tokens[1])\n",
    "    return sentences, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bá»‡nh']\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_tokens = read_data(os.path.join(DATA_PATH, DATA_TYPE+'/train_'+DATA_TYPE+'.conll'))\n",
    "val_sentences, val_tokens = read_data(os.path.join(DATA_PATH, DATA_TYPE+'/dev_'+DATA_TYPE+'.conll'))\n",
    "test_sentences, test_tokens = read_data(os.path.join(DATA_PATH, DATA_TYPE+'/test_'+DATA_TYPE+'.conll'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<pad>': PADDING_TOKEN, '<unk>': UNKNOWN_TOKEN, '<s>': START_SENTENCE, '</s>': END_SENTENCE}\n",
    "        self.idx2word = {value: key for key,value in self.word2idx.items()}\n",
    "        self.length = 4\n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.length\n",
    "            self.idx2word[self.length] = word\n",
    "            self.length+=1\n",
    "    def _encode_sentence(self, sentence):\n",
    "        return [self.word2idx.get(word, UNKNOWN_TOKEN) for word in sentence]\n",
    "    def _decode_sentence(self, tokens):\n",
    "        return [self.idx2word[token] for token in tokens]\n",
    "    def build_vocab(self, sentences):\n",
    "        for sent in sentences:\n",
    "            for token in sent:\n",
    "                self._add_word(token)\n",
    "    def encode(self, sentences):\n",
    "        return [self._encode_sentence(sent) for sent in sentences]\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __call__(self, word):\n",
    "        return self.word2idx[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab()\n",
    "vocab.build_vocab(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, sentences, tokens):\n",
    "        self.sentences = sentences\n",
    "        self.tokens = tokens\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    def __getitem__(self, index):\n",
    "        return self.sentences[index], self.tokens[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityVocab:\n",
    "    def __init__(self):\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        self.length = 0\n",
    "    def _add_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            self.token2idx[token] = self.length\n",
    "            self.idx2token[self.length] = token\n",
    "            self.length+=1\n",
    "    def _encode_token(self, tokens):\n",
    "        return [self.token2idx.get(token, 0) for token in tokens]\n",
    "    def _decode_token(self, tokens):\n",
    "        return [self.idx2token[token] for token in tokens]\n",
    "    def encode(self, tokens):\n",
    "        return [self._encode_token(token) for token in tokens]\n",
    "    def build(self,tokens):\n",
    "        for tok in tokens:\n",
    "            for token in tok:\n",
    "                self._add_token(token)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __call__(self, token):\n",
    "        return self.token2idx[token] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_vocab = EntityVocab()\n",
    "entity_vocab.build(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_encoded, train_entities_encoded = vocab.encode(train_sentences), entity_vocab.encode(train_tokens)\n",
    "val_sentences_encoded, val_entities_encoded = vocab.encode(val_sentences), entity_vocab.encode(val_tokens)\n",
    "test_sentences_encoded, test_entities_encoded = vocab.encode(test_sentences), entity_vocab.encode(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NER_Dataset(train_sentences_encoded, train_entities_encoded)\n",
    "valset = NER_Dataset(val_sentences_encoded, val_entities_encoded)\n",
    "testset = NER_Dataset(test_sentences_encoded, test_entities_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _collate_fn(batch):\n",
    "    sentences, entities = zip(*batch)\n",
    "    max_length = max([len(sentence) for sentence in sentences])\n",
    "    padding_sentences = []\n",
    "    padding_entities = []\n",
    "    for sentence, entity in zip(sentences, entities):\n",
    "        padding_sentences.append(sentence+[PADDING_TOKEN for j in range(max_length-len(sentence))])\n",
    "        padding_entities.append(entity+[PADDING_TOKEN for j in range(max_length-len(sentence))])\n",
    "    sentences = torch.LongTensor(padding_sentences)\n",
    "    entities = torch.LongTensor(padding_entities)\n",
    "    masks = sentences!=PADDING_TOKEN\n",
    "    return sentences, masks, entities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, entity = trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence, entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=_collate_fn)\n",
    "val_loader = DataLoader(valset, batch_size=BATCH_SIZE, collate_fn=_collate_fn)\n",
    "test_loader = DataLoader(testset, batch_size=BATCH_SIZE, collate_fn=_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, output_size, dropout=0.1, n_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=PADDING_TOKEN)\n",
    "        self.lstm = nn.LSTM(input_size = emb_size, hidden_size = hidden_size//2, num_layers = n_layers, batch_first = True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    def forward(self, data, masks):\n",
    "        emb = self.embeddings(data)\n",
    "        output, hidden = self.lstm(emb) \n",
    "        output = self.dropout(output)\n",
    "        output = self.hidden2tag(output)\n",
    "        output = self.softmax(output)\n",
    "        return output.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 104750,\n",
       "         1: 1137,\n",
       "         2: 2545,\n",
       "         3: 1439,\n",
       "         4: 1552,\n",
       "         5: 5398,\n",
       "         6: 2549,\n",
       "         7: 3240,\n",
       "         8: 682,\n",
       "         9: 349,\n",
       "         10: 2500,\n",
       "         11: 205,\n",
       "         12: 5242,\n",
       "         13: 226,\n",
       "         14: 542,\n",
       "         15: 67,\n",
       "         16: 62,\n",
       "         17: 13,\n",
       "         18: 2,\n",
       "         19: 11})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(token for tokens in train_entities_encoded for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_weight(labels_dict):\n",
    "  total = np.sum(list(labels_dict.values()))\n",
    "  keys  = labels_dict.keys()\n",
    "  class_weight = dict()\n",
    "  num_classes = len(labels_dict)\n",
    "  for key in keys:\n",
    "      score = round(total / (num_classes * labels_dict[key]+total/10), 2)\n",
    "      class_weight[key] = score\n",
    "  return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = create_class_weight(counter)\n",
    "#criterion = nn.CrossEntropyLoss(torch.FloatTensor(list(class_weight.values())), reduction='none')\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "model = NER_BiLSTM(len(vocab), EMBEDDING_SIZE, HIDDEN_SIZE, len(entity_vocab), dropout=0.15, n_layers=1)\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_logger(logger):\n",
    "  for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "  for f in logger.filters[:]:\n",
    "    logger.removeFilters(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 23:06:38,081 [MainThread  ] [INFO ]  Experiment start\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)\n",
    "\n",
    "fileHandler = logging.FileHandler(\"{0}/{1}.log\".format('./', f'experiments_{type(model).__name__}'), mode = 'w')\n",
    "fileHandler.setFormatter(logFormatter)\n",
    "rootLogger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "rootLogger.addHandler(consoleHandler)\n",
    "\n",
    "rootLogger.info(\"Experiment start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_epoch(model, criterion, optimizer, dataset, epoch):\n",
    "    model.train()\n",
    "    rootLogger.info(f\"-----------------------Epoch {epoch}--------------------\")\n",
    "    epoch_loss = []\n",
    "    for batch in tqdm(dataset, total=len(dataset), desc = 'Train epoch %s'%epoch):\n",
    "        optimizer.zero_grad()\n",
    "        data, masks, labels = batch\n",
    "        out = model(data, masks)\n",
    "        length = torch.sum(masks)\n",
    "        loss = torch.sum(criterion(out, labels)*masks)/length\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return sum(epoch_loss)/len(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, criterion, dataset):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    true_pred =  []\n",
    "    ground_truth = []\n",
    "    for batch in tqdm(dataset, total=len(dataset), desc= \"Validation\"):\n",
    "        data, masks, labels = batch\n",
    "        out = model(data, masks)\n",
    "        length = torch.sum(masks)\n",
    "        loss = torch.sum(criterion(out, labels)*masks)/length\n",
    "        val_loss.append(loss.item())\n",
    "        prediction = torch.argmax(out, dim=1)\n",
    "        true_pred.append(torch.sum((prediction==labels)*masks))\n",
    "        ground_truth.append(length)\n",
    "    return sum(val_loss)/len(val_loss), sum(true_pred)/sum(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer):\n",
    "    save_point = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    save_file = f'{type(model).__name__}.pt'\n",
    "    torch.save(save_point, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, trainset, valset, epochs):\n",
    "    rootLogger.info(\"###Start training####\")\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, criterion, optimizer, trainset, epoch)\n",
    "        val_loss, val_acc = eval(model, criterion, valset)\n",
    "        rootLogger.info(f\"Train loss: {train_loss:.3f}\\t Val loss: {val_loss:.3f}\\t Val acc: {val_acc:.3f}\")\n",
    "        if val_acc>best_acc:\n",
    "            best_acc = val_acc\n",
    "            save_model(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 23:53:19,562 [MainThread  ] [INFO ]  ###Start training####\n",
      "2022-01-24 23:53:19,564 [MainThread  ] [INFO ]  -----------------------Epoch 0--------------------\n",
      "Train epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:08<00:00,  9.52it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 21.92it/s]\n",
      "2022-01-24 23:53:29,332 [MainThread  ] [INFO ]  Train loss: 0.018\t Val loss: 0.164\t Val acc: 0.961\n",
      "2022-01-24 23:53:29,349 [MainThread  ] [INFO ]  -----------------------Epoch 1--------------------\n",
      "Train epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:08<00:00,  9.28it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 22.80it/s]\n",
      "2022-01-24 23:53:39,279 [MainThread  ] [INFO ]  Train loss: 0.014\t Val loss: 0.169\t Val acc: 0.961\n",
      "2022-01-24 23:53:39,295 [MainThread  ] [INFO ]  -----------------------Epoch 2--------------------\n",
      "Train epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:08<00:00,  9.24it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 21.71it/s]\n",
      "2022-01-24 23:53:49,325 [MainThread  ] [INFO ]  Train loss: 0.011\t Val loss: 0.182\t Val acc: 0.961\n",
      "2022-01-24 23:53:49,326 [MainThread  ] [INFO ]  -----------------------Epoch 3--------------------\n",
      "Train epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:08<00:00,  9.41it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 22.89it/s]\n",
      "2022-01-24 23:53:59,132 [MainThread  ] [INFO ]  Train loss: 0.009\t Val loss: 0.194\t Val acc: 0.960\n",
      "2022-01-24 23:53:59,133 [MainThread  ] [INFO ]  -----------------------Epoch 4--------------------\n",
      "Train epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:08<00:00,  9.29it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 22.76it/s]\n",
      "2022-01-24 23:54:09,051 [MainThread  ] [INFO ]  Train loss: 0.009\t Val loss: 0.194\t Val acc: 0.960\n"
     ]
    }
   ],
   "source": [
    "train(model, criterion, optimizer, train_loader, val_loader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 24.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.16862980474252254, tensor(0.9612))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('NER_BiLSTM.pt')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "eval(model, criterion, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, output_size, dropout=0.1, n_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=PADDING_TOKEN)\n",
    "        self.lstm = nn.LSTM(input_size = emb_size, hidden_size = hidden_size, num_layers = n_layers, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    def forward(self, data, masks):\n",
    "        emb = self.embeddings(data)\n",
    "        output, hidden = self.lstm(emb) \n",
    "        output = self.dropout(output)\n",
    "        output = self.hidden2tag(output)\n",
    "        output = self.softmax(output)\n",
    "        return output.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
